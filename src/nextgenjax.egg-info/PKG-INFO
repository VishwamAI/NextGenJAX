Metadata-Version: 2.1
Name: nextgenjax
Version: 0.1.0
Summary: A JAX-based neural network library surpassing Google DeepMind's Haiku and Optax
Home-page: https://github.com/VishwamAI/NextGenJAX
Author: kasinadhsarma
Author-email: kasinadhsarma@gmail.com
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Provides-Extra: dev
License-File: LICENSE

# NextGenJAX

## Overview
NextGenJAX is an advanced neural network library built on top of JAX, designed to surpass the capabilities of existing libraries such as Google DeepMind's Haiku and Optax. It leverages the flexibility and performance of JAX and Flax to provide a modular, high-performance, and easy-to-use framework for building and training neural networks.

## Features
- Modular design with customizable layers and activation functions
- Support for various optimizers, including custom optimizers
- Flexible training loop with support for custom loss functions
- Integration with JAX and Flax for high performance and scalability
- Comprehensive test suite to ensure model correctness and performance

## Installation
To install NextGenJAX, clone the repository and install the required dependencies:
```bash
git clone https://github.com/VishwamAI/NextGenJAX.git
cd NextGenJAX
pip install -r requirements.txt
```

## Usage
### Creating a Model
To create a model using NextGenJAX, define the layers and activation functions, and initialize the model:
```python
import jax
import jax.numpy as jnp
from src.layers import DenseLayer, ConvolutionalLayer
from src.custom_layers import CustomLayer
from src.model import NextGenModel

# Define the layers
layers = [
    DenseLayer(features=128, activation=jnp.relu),
    ConvolutionalLayer(features=64, kernel_size=(3, 3), activation=jnp.relu),
    CustomLayer(features=10, activation=jnp.tanh)
]

# Initialize the model
model = NextGenModel(layers=layers)
```

### Training the Model
To train the model, use the training loop provided in `train.py`:
```python
from src.train import create_train_state, train_model
from src.optimizers import sgd, adam

# Define the optimizer
optimizer = adam(learning_rate=0.001)

# Create the training state
train_state = create_train_state(model, optimizer)

# Define the training data and loss function
train_data = ...  # Your training data here
loss_fn = ...  # Your loss function here

# Train the model
train_model(train_state, train_data, loss_fn, num_epochs=10)
```

## Contributing
We welcome contributions to NextGenJAX! If you would like to contribute, please follow these steps:
1. Fork the repository
2. Create a new branch (`git checkout -b feature/your-feature`)
3. Make your changes and commit them (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature/your-feature`)
5. Create a new pull request using the [Pull Request Template](.github/PULL_REQUEST_TEMPLATE.md)

For more detailed guidelines, please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file.

## Reporting Issues
If you encounter any issues or have suggestions for improvements, please open an issue in the repository. Use the appropriate issue template:
- [Bug Report](.github/ISSUE_TEMPLATE/bug_report.md)
- [Feature Request](.github/ISSUE_TEMPLATE/feature_request.md)

Provide as much detail as possible to help us understand and address the problem.

## License
NextGenJAX is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.

## Acknowledgements
NextGenJAX is inspired by the work of Google DeepMind and the JAX and Flax communities. We thank them for their contributions to the field of machine learning.

Last updated: 2023-05-10 12:00:00 UTC

